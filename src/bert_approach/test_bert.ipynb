{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "221603e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Set up autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8363348",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "23b9c5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\eggle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eggle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from word_normalization import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from process_input import *\n",
    "from bert import BERT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Download TextBlob dependencies\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a12f9",
   "metadata": {},
   "source": [
    "### Define Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0289f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/'\n",
    "\n",
    "# Tf-idf params\n",
    "MAX_VOCAB = 10000\n",
    "MIN_FREQ = 1\n",
    "\n",
    "# Model params\n",
    "MAX_LEN = 100\n",
    "D_MODEL = 384     \n",
    "D_FF = 1024         \n",
    "HEADS = 4         \n",
    "N = 4             \n",
    "LR = 2e-4           \n",
    "DROPOUT = 0.2     \n",
    "LABEL_SMOOTH = 0 \n",
    "\n",
    "# Training params\n",
    "BATCH_SIZE = 128  \n",
    "EPOCHS = 30       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac020439",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a02d03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'twitter_sentiment_train.csv'))\n",
    "df_test  = pd.read_csv(os.path.join(DATA_PATH, 'twitter_sentiment_test.csv'))\n",
    "\n",
    "# Shuffle train set\n",
    "RANDOM_STATE = 123\n",
    "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd169210",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "1. Word normalization\n",
    "2. Tokenize sentence\n",
    "3. Finally mark each token to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4c51cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Vocabulary\n",
    "# vocab_path = \"./vocab.json\"\n",
    "# vocab = {}\n",
    "# # Check if vocabulary already exists\n",
    "# if os.path.exists(vocab_path):\n",
    "#     # Read json and save\n",
    "#     with open(vocab_path, \"r\") as f:\n",
    "#         vocab = json.load(f)\n",
    "# else:\n",
    "#     # Put special tokens\n",
    "#     vocab = {\n",
    "#         '<pad>': 0,   # Padding token\n",
    "#         '<cls>': 1,   # Classification token (start of sequence)\n",
    "#         '<sep>': 2    # Separator token (end of sequence)\n",
    "#     }\n",
    "\n",
    "#     index = 3\n",
    "#     for tweet in df_train['text']:\n",
    "#         # Pre-process input\n",
    "#         tokens = preprocessing_text(tweet)\n",
    "#         # Update vocabulary\n",
    "#         for token in tokens:\n",
    "#             if token not in vocab.keys():\n",
    "#                 vocab[token] = index\n",
    "#                 index += 1\n",
    "#     for tweet in df_test['text']:\n",
    "#         # Pre-process input\n",
    "#         tokens = preprocessing_text(tweet)\n",
    "#         # Update vocabulary\n",
    "#         for token in tokens:\n",
    "#             if token not in vocab.keys():\n",
    "#                 vocab[token] = index\n",
    "#                 index += 1\n",
    "                \n",
    "#     with open(vocab_path, 'w') as f:\n",
    "#         json.dump(vocab, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76648f4",
   "metadata": {},
   "source": [
    "### Create Vocabulary Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dd4be238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_tfidf_vocabulary(df_train, df_test, max_features=5000, min_df=2):\n",
    "#     \"\"\"\n",
    "#     Build vocabulary using TF-IDF's most discriminative features.\n",
    "#     Preserves emojis and adds sentiment words.\n",
    "#     \"\"\"\n",
    "#     print(\"=\"*70)\n",
    "#     print(\"BUILDING TF-IDF-BASED VOCABULARY\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # Step 1: Preprocess all texts\n",
    "#     print(\"\\n1. Preprocessing texts...\")\n",
    "#     train_texts = []\n",
    "#     test_texts = []\n",
    "#     all_emojis = set()\n",
    "    \n",
    "#     for text in df_train['text']:\n",
    "#         tokens = preprocessing_text(text)\n",
    "#         # Separate emojis from words\n",
    "#         emojis = [t for t in tokens if emoji.is_emoji(t)]\n",
    "#         words = [t for t in tokens if not emoji.is_emoji(t)]\n",
    "#         all_emojis.update(emojis)\n",
    "#         train_texts.append(' '.join(words))  # TF-IDF on words only\n",
    "    \n",
    "#     for text in df_test['text']:\n",
    "#         tokens = preprocessing_text(text)\n",
    "#         emojis = [t for t in tokens if emoji.is_emoji(t)]\n",
    "#         words = [t for t in tokens if not emoji.is_emoji(t)]\n",
    "#         all_emojis.update(emojis)\n",
    "#         test_texts.append(' '.join(words))\n",
    "    \n",
    "#     print(f\"   Found {len(all_emojis)} unique emojis\")\n",
    "    \n",
    "#     # Step 2: Fit TF-IDF on words (not emojis)\n",
    "#     print(f\"\\n2. Fitting TF-IDF (max_features={max_features}, min_df={min_df})...\")\n",
    "    \n",
    "#     tfidf = TfidfVectorizer(\n",
    "#         max_features=max_features,\n",
    "#         min_df=min_df,\n",
    "#         max_df=0.90,\n",
    "#         ngram_range=(1, 1),  # Unigrams only\n",
    "#         token_pattern=r'\\S+',\n",
    "#         lowercase=False\n",
    "#     )\n",
    "    \n",
    "#     tfidf.fit(train_texts + test_texts)\n",
    "#     tfidf_words = set(tfidf.get_feature_names_out())\n",
    "    \n",
    "#     print(f\"   TF-IDF selected {len(tfidf_words)} word features\")\n",
    "    \n",
    "#     # Step 3: Add sentiment words (ensure important words are included)\n",
    "#     print(f\"\\n3. Adding critical sentiment words...\")\n",
    "    \n",
    "#     sentiment_words = {\n",
    "#         # Positive\n",
    "#         'good', 'great', 'love', 'best', 'amazing', 'awesome', 'excellent',\n",
    "#         'perfect', 'wonderful', 'fantastic', 'happy', 'beautiful',\n",
    "#         # Negative\n",
    "#         'bad', 'hate', 'worst', 'terrible', 'awful', 'horrible', 'poor',\n",
    "#         'disappointing', 'sad', 'angry', 'useless', 'waste',\n",
    "#         # Negations (CRITICAL!)\n",
    "#         'not', 'no', 'never', 'neither', 'nor', 'cannot', 'hardly',\n",
    "#         # Intensifiers\n",
    "#         'very', 'really', 'so', 'too', 'extremely'\n",
    "#     }\n",
    "    \n",
    "#     added_sentiment = 0\n",
    "#     for word in sentiment_words:\n",
    "#         if word not in tfidf_words:\n",
    "#             tfidf_words.add(word)\n",
    "#             added_sentiment += 1\n",
    "    \n",
    "#     print(f\"   Added {added_sentiment} critical sentiment words\")\n",
    "    \n",
    "#     # Step 4: Build final vocabulary\n",
    "#     print(f\"\\n4. Building final vocabulary...\")\n",
    "    \n",
    "#     vocab = {\n",
    "#         '<pad>': 0,\n",
    "#         '<cls>': 1,\n",
    "#         '<sep>': 2\n",
    "#     }\n",
    "    \n",
    "#     index = 3\n",
    "    \n",
    "#     # Add TF-IDF words\n",
    "#     for word in sorted(tfidf_words):\n",
    "#         vocab[word] = index\n",
    "#         index += 1\n",
    "    \n",
    "#     # Add ALL emojis from dataset\n",
    "#     for emoji_char in sorted(all_emojis):\n",
    "#         if emoji_char not in vocab:\n",
    "#             vocab[emoji_char] = index\n",
    "#             index += 1\n",
    "    \n",
    "#     # Add special tokens\n",
    "#     special_tokens = ['url', 'email']\n",
    "#     for token in special_tokens:\n",
    "#         if token not in vocab:\n",
    "#             vocab[token] = index\n",
    "#             index += 1\n",
    "    \n",
    "#     # Calculate coverage\n",
    "#     print(f\"\\n5. Calculating coverage...\")\n",
    "    \n",
    "#     total_tokens = 0\n",
    "#     covered_tokens = 0\n",
    "    \n",
    "#     for text in train_texts + test_texts:\n",
    "#         tokens = text.split()\n",
    "#         total_tokens += len(tokens)\n",
    "#         covered_tokens += sum(1 for t in tokens if t in vocab)\n",
    "    \n",
    "#     return vocab, tfidf\n",
    "\n",
    "# # Build vocabulary\n",
    "# vocab_path = \"./vocab_tfidf.json\"\n",
    "\n",
    "# if os.path.exists(vocab_path):\n",
    "#     print(\"Loading existing TF-IDF vocabulary...\")\n",
    "#     with open(vocab_path, \"r\") as f:\n",
    "#         vocab = json.load(f)\n",
    "#     print(f\"Loaded vocabulary: {len(vocab)} tokens\")\n",
    "# else:\n",
    "#     vocab, tfidf_vectorizer = build_tfidf_vocabulary(\n",
    "#         df_train,\n",
    "#         df_test,\n",
    "#         max_features=MAX_VOCAB,\n",
    "#         min_df=MIN_FREQ\n",
    "#     )\n",
    "    \n",
    "#     # Save vocabulary\n",
    "#     with open(vocab_path, 'w') as f:\n",
    "#         json.dump(vocab, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f796cb",
   "metadata": {},
   "source": [
    "### Create Vocabulary with Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "14fa8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TextBlob vocabulary: 6589 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simply load the vocabulary\n",
    "vocab_path = \"./vocab_textblob.json\"\n",
    "\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    print(f\"Loaded TextBlob vocabulary: {len(vocab)} tokens\")\n",
    "else:\n",
    "    # Build it\n",
    "    from textblob_vocabulary import build_textblob_vocabulary\n",
    "    vocab, _ = build_textblob_vocabulary(df_train, df_test, max_vocab=MAX_VOCAB)\n",
    "    \n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump(vocab, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b0c44",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "1. Pass each sentence to model:\n",
    "    - Transform index to embeddings\n",
    "    - Forward to layers \n",
    "    - Final MLP head for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "60fcc4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = BERT(\n",
    "        vocab_size=len(vocab.keys()),\n",
    "        max_len=MAX_LEN,\n",
    "        d_model=D_MODEL, \n",
    "        d_ff=D_FF, \n",
    "        num_heads=HEADS, \n",
    "        N=N, \n",
    "        lr=LR, \n",
    "        dropout=DROPOUT,\n",
    "        num_classes=2\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "# Initialize scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS,      \n",
    "    eta_min=1e-9   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a05ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|███| 111/111 [00:24<00:00,  4.56it/s, loss=0.7111, acc=52.11%, lr=2.00e-04]\n",
      "Epoch 1/30 [Test]: 100%|███████████████████| 50/50 [00:12<00:00,  4.01it/s, loss=0.6354, acc=66.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/30 Summary:\n",
      "  Train Loss:     1.4376\n",
      "  Train Accuracy: 52.11% (7392/14186)\n",
      "  Test Loss:      0.6375\n",
      "  Test Accuracy:  66.93% (4248/6347)\n",
      "  GPU Memory: 0.34 GB allocated, 1.60 GB reserved\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|███| 111/111 [00:38<00:00,  2.87it/s, loss=0.5588, acc=63.41%, lr=1.99e-04]\n",
      "Epoch 2/30 [Test]: 100%|███████████████████| 50/50 [00:14<00:00,  3.52it/s, loss=0.5523, acc=72.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 2/30 Summary:\n",
      "  Train Loss:     0.6442\n",
      "  Train Accuracy: 63.41% (8995/14186)\n",
      "  Test Loss:      0.5432\n",
      "  Test Accuracy:  72.70% (4614/6347)\n",
      "  GPU Memory: 0.34 GB allocated, 0.81 GB reserved\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|███| 111/111 [00:36<00:00,  3.05it/s, loss=0.4654, acc=70.72%, lr=1.98e-04]\n",
      "Epoch 3/30 [Test]: 100%|███████████████████| 50/50 [00:05<00:00,  9.05it/s, loss=0.4996, acc=75.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 3/30 Summary:\n",
      "  Train Loss:     0.5643\n",
      "  Train Accuracy: 70.72% (10033/14186)\n",
      "  Test Loss:      0.5062\n",
      "  Test Accuracy:  75.09% (4766/6347)\n",
      "  GPU Memory: 0.34 GB allocated, 0.80 GB reserved\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|███| 111/111 [00:41<00:00,  2.70it/s, loss=0.4505, acc=74.47%, lr=1.95e-04]\n",
      "Epoch 4/30 [Test]: 100%|███████████████████| 50/50 [00:11<00:00,  4.47it/s, loss=0.4886, acc=75.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 4/30 Summary:\n",
      "  Train Loss:     0.5164\n",
      "  Train Accuracy: 74.47% (10564/14186)\n",
      "  Test Loss:      0.4892\n",
      "  Test Accuracy:  75.93% (4819/6347)\n",
      "  GPU Memory: 0.34 GB allocated, 0.77 GB reserved\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]:  78%|███▏| 87/111 [00:32<00:40,  1.68s/it, loss=0.4422, acc=76.50%, lr=1.91e-04]"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    # ==================== TRAINING ====================\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    num_train_batches = (len(df_train) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    pbar = tqdm(range(0, len(df_train), BATCH_SIZE), \n",
    "                desc=f\"Epoch {e+1}/{EPOCHS} [Train]\", \n",
    "                total=num_train_batches,\n",
    "                ncols=100)\n",
    "    \n",
    "    for i in pbar:\n",
    "        # Get batch\n",
    "        batch_texts = df_train['text'].iloc[i:min(i+BATCH_SIZE, len(df_train))]\n",
    "        batch_labels = df_train['label'].iloc[i:min(i+BATCH_SIZE, len(df_train))]\n",
    "        \n",
    "        # Process batch\n",
    "        input_ids, attention_masks, labels = process_batch(\n",
    "            batch_texts, batch_labels, vocab, MAX_LEN, device\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_masks)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Free memory every batch\n",
    "        del input_ids, attention_masks, labels, outputs, predicted\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.0 * correct_train / total_train:.2f}%',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "    \n",
    "    scheduler.step()\n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = epoch_loss / num_batches\n",
    "    train_accuracy = 100.0 * correct_train / total_train\n",
    "    \n",
    "    # ==================== TESTING ====================\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    num_test_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_test_batches_total = (len(df_test) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        test_pbar = tqdm(range(0, len(df_test), BATCH_SIZE),\n",
    "                        desc=f\"Epoch {e+1}/{EPOCHS} [Test]\",\n",
    "                        total=num_test_batches_total,\n",
    "                        ncols=100)\n",
    "        \n",
    "        for i in test_pbar:\n",
    "            batch_texts = df_test['text'].iloc[i:min(i+BATCH_SIZE, len(df_test))]\n",
    "            batch_labels = df_test['label'].iloc[i:min(i+BATCH_SIZE, len(df_test))]\n",
    "            \n",
    "            input_ids, attention_masks, labels = process_batch(\n",
    "                batch_texts, batch_labels, vocab, MAX_LEN, device\n",
    "            )\n",
    "            \n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            num_test_batches += 1\n",
    "            \n",
    "            # Free memory\n",
    "            del input_ids, attention_masks, labels, outputs, predicted\n",
    "            \n",
    "            test_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.0 * correct_test / total_test:.2f}%'\n",
    "            })\n",
    "    \n",
    "    # Clear cache after testing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    avg_test_loss = test_loss / num_test_batches\n",
    "    test_accuracy = 100.0 * correct_test / total_test\n",
    "    \n",
    "    # ==================== EPOCH SUMMARY ====================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {e+1}/{EPOCHS} Summary:\")\n",
    "    print(f\"  Train Loss:     {avg_train_loss:.4f}\")\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.2f}% ({correct_train}/{total_train})\")\n",
    "    print(f\"  Test Loss:      {avg_test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_accuracy:.2f}% ({correct_test}/{total_test})\")\n",
    "    \n",
    "    # Print memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"  GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ==================== CLEANUP ====================\n",
    "print(\"Training complete! Cleaning up memory...\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clear Python garbage\n",
    "gc.collect()\n",
    "\n",
    "print(\"✓ Memory cleaned!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
